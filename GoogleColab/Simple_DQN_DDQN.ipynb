{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["jpsPEYD0eZQj","uvegX0cmwcnd","iDkSAV-cwiXp","JYs5I-mw8e46","Qp7dU2LN8igv"],"authorship_tag":"ABX9TyOL9nSPqSGtApAg4J1L5x4Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Drive mount - It's not mendatory"],"metadata":{"id":"jpsPEYD0eZQj"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPa0YsFCrSwg","executionInfo":{"status":"ok","timestamp":1704689786860,"user_tz":-540,"elapsed":26497,"user":{"displayName":"­오다현 / 학생 / 항공우주공학과","userId":"08036004447310994075"}},"outputId":"2572d949-a73d-447f-8fdb-e396e9cae91b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## If you want unmount it, please follow below"],"metadata":{"id":"KaZrtId9h7ud"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.flush_and_unmount()"],"metadata":{"id":"lWqqx8u6hXxS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define Environment and utils"],"metadata":{"id":"uvegX0cmwcnd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uS3kOjkBvJpT"},"outputs":[],"source":["import numpy as np\n","\n","class TwoDimArrayMap:\n","    def __init__(self, x_dim, y_dim, action_space_dim=4):\n","        self.maze = np.zeros([x_dim, y_dim])\n","        self.success_reward = 0\n","        self.failed_reward = -1\n","\n","        self.reward_states = np.full((x_dim, y_dim), self.failed_reward)    # make full of self.failed_reward matrix as x_dim * y_dim\n","        self.state = np.array([0, 0])\n","\n","        self.observation_space_dim = x_dim * y_dim\n","        self.action_space_dim = action_space_dim\n","\n","        self.row = len(self.maze)\n","        self.col = len(self.maze[0])\n","\n","        self.goal = np.array([self.row - 1, 0])\n","        self.reward_states[self.goal[0]][self.goal[1]] = self.success_reward\n","\n","    def SimpleMazation(self):   # simple maze having one large wall. Represent wall as 1. At the reward states, wall is -9\n","        for i in range(self.row):\n","            for j in range(self.col):\n","                if (self.row//3) <= i < (2 * self.row//3):\n","                    if j < self.col * (2/3):\n","                        self.maze[i][j] = 1\n","                        self.reward_states[i][j] = -9\n","\n","    def reset(self):\n","        self.state = np.array([0, 0])\n","        return self.state\n","\n","    def step(self, action):\n","        if action == 0 and self.state[1] < self.col-1:\n","            if self.maze[self.state[0]][self.state[1]+1] == 0:  # move to right\n","                self.state[1] += 1\n","        elif action == 1 and self.state[1] > 0:\n","            if self.maze[self.state[0]][self.state[1]-1] == 0:  # move to left\n","                self.state[1] -= 1\n","        elif action == 2 and self.state[0] < self.row-1:\n","            if self.maze[self.state[0]+1][self.state[1]] == 0:  # move to down\n","                self.state[0] += 1\n","        elif action == 3 and self.state[0] > 0:\n","            if self.maze[self.state[0]-1][self.state[1]] == 0:  # move to up\n","                self.state[0] -= 1\n","\n","        if (self.state[0] == self.goal[0]) and (self.state[1] == self.goal[1]):\n","            reward = self.success_reward\n","            done = True\n","        else:\n","            reward = self.failed_reward\n","            done = False\n","\n","        return self.state, reward, done"]},{"cell_type":"markdown","source":["## Define ReplayMemory"],"metadata":{"id":"XtAqhvRAwt3h"}},{"cell_type":"code","source":["from collections import deque, namedtuple\n","import random\n","\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.memory = deque([], maxlen=capacity)\n","        self.Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'mask'))\n","\n","    def push(self, *args):\n","        \"\"\"Saves a transition.\"\"\"\n","        self.memory.append(self.Transition(*args))\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"metadata":{"id":"ViTYTlOwwxyD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define utils"],"metadata":{"id":"t1RON8rTw39x"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","class OneLineToCell:\n","    def __init__(self, x_dim, y_dim):\n","        self.maze = np.zeros([x_dim, y_dim])\n","        self.state = 0\n","        self.observation_space_dim = self.maze.size\n","        self.row = len(self.maze)\n","        self.col = len(self.maze[0])\n","\n","    def FillGridByOneLineArray(self, array):\n","        for i in range(self.row):\n","            for j in range(self.col):\n","                self.maze[i][j] = array[i*self.row + j]\n","        return self.maze"],"metadata":{"id":"AN2pWmeiw6CU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define saveing train and test history as txt"],"metadata":{"id":"ZjC_JwQj8PlX"}},{"cell_type":"code","source":["import os\n","import datetime\n","import numpy as np\n","\n","def save_history_txt(SAVE, device, path, Q_net, QnetToCell, env, i_episode, t):\n","    if i_episode % SAVE == 0:\n","        V_table, Action_table = QnetToCell.FillGridByQnet(Q_net, env, device)\n","        print(f\"--------{i_episode} is saved with {t} steps. V_table and Action table\")\n","\n","        if not os.path.isdir(path+'/V_table_train') or not os.path.isdir(path+'/Action_table_train'):\n","            os.makedirs(path+'/V_table_train')\n","            os.makedirs(path+'/Action_table_train')\n","\n","        now = datetime.datetime.now().strftime(\"%m-%d_%H:%M:%S\")\n","        np.savetxt(f'{path}/V_table_train/V_table_{i_episode}_{now}.txt', V_table, fmt='%.3f')\n","        np.savetxt(f'{path}/Action_table_train/Action_table_{i_episode}_{now}.txt', Action_table, fmt='%d')"],"metadata":{"id":"W244zYje8VU5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define Model\n"],"metadata":{"id":"iDkSAV-cwiXp"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class QNET(nn.Module):\n","\n","    def __init__(self, input_size, output_size):\n","        super(QNET, self).__init__()\n","        self.LReLU = nn.LeakyReLU(0.01)\n","        self.fc1 = nn.Linear(input_size, 128)\n","        self.fc2 = nn.Linear(128, 128)\n","        self.fc3 = nn.Linear(128, output_size)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n","        nn.init.xavier_uniform_(self.fc2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n","        nn.init.xavier_uniform_(self.fc3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n","\n","    def forward(self, x):\n","        x = self.LReLU(self.fc1(x))\n","        x = self.LReLU(self.fc2(x))\n","        x = self.LReLU(self.fc3(x))\n","        return x"],"metadata":{"id":"i5zYBMiewoAK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DQN optimizing code"],"metadata":{"id":"FeTHO1nAxDKu"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","def optimize_model_DQN(buffer, BATCH_SIZE, Q_net, target_Q_net, optimizer, GAMMA):\n","    if len(buffer) < BATCH_SIZE:\n","        return\n","    transitions = buffer.sample(BATCH_SIZE)\n","\n","    batch = buffer.Transition(*zip(*transitions))\n","\n","    state_batch = torch.stack(batch.state)\n","    action_batch = torch.stack(batch.action).unsqueeze(-1)\n","    reward_batch = torch.stack(batch.reward)\n","    non_final_next_states = torch.stack(batch.next_state)\n","    mask_batch = torch.stack(batch.mask)\n","\n","    Q_values = Q_net(state_batch).gather(1, action_batch)   # q(s,a)\n","\n","    with torch.no_grad():\n","        next_state_Q_values_array = target_Q_net(non_final_next_states).max(1)[0]                       # max_a(q_target(s',a))\n","        expected_Q_values_array = (next_state_Q_values_array.mul(mask_batch) * GAMMA) + reward_batch    # r + gamma * max_a(q_target(s',a)) * mask_batch\n","\n","    criterion = nn.MSELoss()\n","    loss = criterion(Q_values, expected_Q_values_array.unsqueeze(-1))\n","\n","    optimizer.zero_grad()               # optimizer reset\n","    loss.backward()                     # calculate backprop\n","    for param in Q_net.parameters():\n","        param.grad.data.clamp_(-1, 1)   # clamp parameters of the network\n","    optimizer.step()                    # apply backprop\n","\n","    return loss"],"metadata":{"id":"WmKjWtQA7-WE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DDQN optimizing code"],"metadata":{"id":"Nac76gh-8FaO"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","def optimize_model_DDQN(buffer, BATCH_SIZE, Q_net, target_Q_net, optimizer, GAMMA):\n","    if len(buffer) < BATCH_SIZE:\n","        return\n","    transitions = buffer.sample(BATCH_SIZE)\n","\n","    batch = buffer.Transition(*zip(*transitions))\n","\n","    state_batch = torch.stack(batch.state)\n","    action_batch = torch.stack(batch.action).unsqueeze(-1)\n","    reward_batch = torch.stack(batch.reward)\n","    non_final_next_states = torch.stack(batch.next_state)\n","    mask_batch = torch.stack(batch.mask)\n","\n","    Q_values = Q_net(state_batch).gather(1, action_batch)   # q(s,a)\n","    argmax_Q_values = Q_net(non_final_next_states).max(1)[1].unsqueeze(-1)  # argmax_a' q(s',a')\n","    with torch.no_grad():\n","        next_state_Q_values_array = target_Q_net(non_final_next_states).gather(1, argmax_Q_values).view(BATCH_SIZE)      # q_target(s',argmax_a' q(s', a'))\n","        expected_Q_values_array = (next_state_Q_values_array.mul(mask_batch) * GAMMA) + reward_batch    # r + gamma * max_a(q_target(s',a)) * mask_batch\n","\n","    criterion = nn.MSELoss()\n","    loss = criterion(Q_values, expected_Q_values_array.unsqueeze(-1))\n","\n","    optimizer.zero_grad()               # optimizer reset\n","    loss.backward()                     # calculate backprop\n","    for param in Q_net.parameters():\n","        param.grad.data.clamp_(-1, 1)   # clamp parameters of the network\n","    optimizer.step()                    # apply backprop\n","\n","    return loss"],"metadata":{"id":"-yJc-UMv8HlB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define train.py and test.py"],"metadata":{"id":"JYs5I-mw8e46"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","def train(TIME_LIMIT, TARGET_UPDATE, steps_done, device, path, Q_net, target_Q_net, buffer, select_action, optimize_model, env):\n","    state = env.reset()\n","    ### reward_states_reset check\n","    np.savetxt(f'{path}/SimpleMaze_Reward_table_reset.txt', env.reward_states, fmt='%d')\n","\n","    state = torch.tensor(state, device=device, dtype=torch.float32)\n","\n","    for t in range(1, TIME_LIMIT+1): # t = 1 ~ TIME_LIMIT\n","        action = select_action(state, test=False)\n","        next_state, reward, done = env.step(action.item())  # action.item() are the pure values from the tensor\n","        reward = torch.tensor(reward,dtype=torch.float32 ,device=device)\n","        next_state = torch.tensor(next_state, device=device, dtype=torch.float32)\n","\n","        buffer.push(state, action, next_state, reward, torch.tensor(1-int(done), device=device, dtype=torch.float32))\n","        state = next_state\n","\n","        loss = optimize_model()\n","\n","        if steps_done % TARGET_UPDATE == 0:\n","            q_target_state_dict = target_Q_net.state_dict()\n","            q_state_dict = Q_net.state_dict()\n","            for key in q_state_dict:\n","                q_target_state_dict[key] = q_state_dict[key]*0.5 + q_target_state_dict[key]*(1-0.5) # 0.5 is the tau value, soft update\n","            target_Q_net.load_state_dict(q_target_state_dict)\n","        if done:\n","            break\n","    return t,done,loss"],"metadata":{"id":"CZF00JQ-8gZn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import datetime\n","import numpy as np\n","\n","def test(X_SIZE, Y_SIZE, TIME_LIMIT, TEST_EPISODES, device, path, writer, Q_net, QnetToCell, select_action, env, i_episode, t):\n","    if i_episode % 100 == 0:\n","        done_stack = 0\n","        steps_stack = 0\n","\n","        for test_episode in range(1, TEST_EPISODES+1):\n","            state = env.reset()\n","            state = torch.tensor(state, device=device, dtype=torch.float32)\n","\n","            for test_t in range(1, TIME_LIMIT+1):\n","                action = select_action(state, test=True)\n","                next_state, _, done = env.step(action.item())\n","                next_state = torch.tensor(next_state, device=device, dtype=torch.float32)\n","                state = next_state\n","                if done:\n","                    break\n","            done_stack += int(done)\n","            steps_stack += test_t\n","\n","            writer.add_scalar('success_rate/test', done_stack/TEST_EPISODES, i_episode)\n","            writer.add_scalar('steps_per_episode/test', steps_stack/TEST_EPISODES, i_episode)\n","            V_table, Action_table = QnetToCell.FillGridByQnet(Q_net, env, device)\n","\n","            if not os.path.isdir(path+'/V_table_test') or not os.path.isdir(path+'/Action_table_test'):\n","                    os.makedirs(path+'/V_table_test')\n","                    os.makedirs(path+'/Action_table_test')\n","\n","            now = datetime.datetime.now().strftime(\"%m-%d_%H:%M:%S\")\n","            np.savetxt(f'{path}/V_table_test/V_table_test_{now}.txt', V_table, fmt='%.3f')\n","            np.savetxt(f'{path}/Action_table_test/Action_table_test_{now}.txt', Action_table, fmt='%d')"],"metadata":{"id":"uxuzCg0n8dIu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define main.py"],"metadata":{"id":"Qp7dU2LN8igv"}},{"cell_type":"code","source":["# code source from https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html\n","\n","import os\n","import math\n","import random\n","import argparse\n","import datetime\n","import numpy as np\n","\n","import torch\n","import torch.optim as optim\n","\n","from torch.utils.tensorboard import SummaryWriter\n","\n","X_SIZE          = 6\n","Y_SIZE          = 6\n","\n","STATE_DIM       = 2\n","ACTION_DIM      = 4\n","\n","NUM_EPISODES    = 1000 #\n","TIME_LIMIT      = 2*X_SIZE*Y_SIZE\n","TEST_EPISODES   = 10\n","\n","TARGET_UPDATE   = NUM_EPISODES//10\n","\n","RM_SIZE             = 1000000\n","BATCH_SIZE          = 2048\n","GAMMA               = 0.9\n","EPS_START           = 0.7\n","EPS_END             = 0.15\n","EPS_DECAY           = NUM_EPISODES\n","\n","SAVE            = NUM_EPISODES//10\n","\n","#### STEPS DONE\n","steps_done = 0\n","\n","#### Argument parser\n","parser = argparse.ArgumentParser()\n","\n","#### CHOOSE DEVICE AND MODEL\n","parser.add_argument('--gpu', type=str, default='0', help='GPU ID')\n","parser.add_argument('--model', type=str, default='DQN', help='DQN or DDQN')\n","args = parser.parse_args(args=['--model', 'DQN'])\n","\n","device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')\n","\n","#### logging\n","now_day = datetime.datetime.now().strftime(\"%m-%d\")\n","now = datetime.datetime.now().strftime(\"%m-%d_%H:%M:%S\")\n","\n","path = f'./results_{args.model}/{now_day}_{X_SIZE, Y_SIZE}_GAM_{GAMMA}_NE_{NUM_EPISODES}_TU_{TARGET_UPDATE}_END_{EPS_END}_{RM_SIZE}_BS_{BATCH_SIZE}/result_{now}'\n","writer = SummaryWriter(f'{path}/tensorboard_{now}')\n","\n","#### Networks\n","Q_net = QNET(STATE_DIM, ACTION_DIM).to(device)\n","target_Q_net = QNET(STATE_DIM, ACTION_DIM).to(device)\n","target_Q_net.load_state_dict(Q_net.state_dict())\n","target_Q_net.eval()\n","\n","#### optimizer\n","optimizer = optim.Adam(Q_net.parameters())   # Optimizer should only work in Q_net\n","buffer = ReplayMemory(RM_SIZE)\n","\n","#### QnetToCell bug fix\n","class QnetToCell:\n","    def __init__(self, x_dim, y_dim):\n","        self.V_states = np.zeros([x_dim, y_dim])\n","        self.Action = np.zeros([x_dim, y_dim])\n","        self.row = len(self.V_states)\n","        self.col = len(self.V_states[0])\n","\n","    def FillGridByQnet(self, Qnet, env, device):\n","        for i in range(self.row):\n","            for j in range(self.col):\n","                state = np.array([i, j])\n","                torch_state = torch.tensor(state, dtype=torch.float32, device=device)\n","\n","                self.V_states[i][j] = Qnet(torch_state).max(0)[0].item()\n","                self.Action[i][j] = Qnet(torch_state).max(0)[1].item()\n","        return self.V_states, self.Action\n","\n","QnetToCell = QnetToCell(X_SIZE, Y_SIZE)\n","\n","def select_action(state, test):\n","    global steps_done\n","    if test:\n","        return Q_net(state).max(0)[1]\n","    else:\n","        sample = random.random()    # random value b/w 0 ~ 1\n","        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n","            math.exp(-1. * steps_done / EPS_DECAY)\n","        steps_done += 1\n","        if sample > eps_threshold:\n","            with torch.no_grad():\n","                return Q_net(state).max(0)[1]\n","        else:\n","            return torch.tensor(random.randrange(ACTION_DIM), device=device, dtype=torch.int64)\n","\n","def optimize_model():\n","    if args.model == 'DQN':\n","        return optimize_model_DQN(buffer, BATCH_SIZE, Q_net, target_Q_net, optimizer, GAMMA)\n","    else:\n","        return optimize_model_DDQN(buffer, BATCH_SIZE, Q_net, target_Q_net, optimizer, GAMMA)\n","\n","if __name__ == '__main__':\n","\n","    env = TwoDimArrayMap(X_SIZE, Y_SIZE)\n","    env.SimpleMazation()\n","\n","    #### Maze logging\n","    if not os.path.isdir(path):\n","        os.makedirs(path)\n","    np.savetxt(f'{path}/SimpleMaze_table.txt', env.maze, fmt='%d')\n","    np.savetxt(f'{path}/SimpleMaze_Reward_table.txt', env.reward_states, fmt='%d')\n","    print(f'{path} is running...')\n","\n","    #### Training\n","    done_stack = 0\n","    steps_stack = 0\n","\n","    for i_episode in range(1, NUM_EPISODES+1):\n","\n","        t, done, loss = train(TIME_LIMIT, TARGET_UPDATE, steps_done, device, path, Q_net, target_Q_net, buffer, select_action, optimize_model, env)\n","\n","        ### tensorboard\n","        if loss is not None:\n","            writer.add_scalar('success_rate/train', int(done), i_episode)\n","            writer.add_scalar('steps_per_episode/train', t, i_episode)\n","            writer.add_scalar('Loss/train', loss, i_episode)\n","\n","        test(X_SIZE, Y_SIZE, TIME_LIMIT, TEST_EPISODES, device, path, writer, Q_net, QnetToCell, select_action, env, i_episode, t)\n","\n","        save_history_txt(SAVE, device, path, Q_net, QnetToCell, env, i_episode, t)\n","\n","    writer.close()\n","    print(f'{path} is done')\n","    print('Complete')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ajb9cQr38j1j","executionInfo":{"status":"ok","timestamp":1704696045207,"user_tz":-540,"elapsed":464046,"user":{"displayName":"­오다현 / 학생 / 항공우주공학과","userId":"08036004447310994075"}},"outputId":"793d3fd3-ae4c-4b14-8e92-b70a2d016d39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["./results_DQN/01-08_(6, 6)_GAM_0.9_NE_1000_TU_100_END_0.15_1000000_BS_2048/result_01-08_06:33:01 is running...\n","--------100 is saved with 72 steps. V_table and Action table\n","--------200 is saved with 16 steps. V_table and Action table\n","--------300 is saved with 13 steps. V_table and Action table\n","--------400 is saved with 14 steps. V_table and Action table\n","--------500 is saved with 24 steps. V_table and Action table\n","--------600 is saved with 18 steps. V_table and Action table\n","--------700 is saved with 13 steps. V_table and Action table\n","--------800 is saved with 13 steps. V_table and Action table\n","--------900 is saved with 17 steps. V_table and Action table\n","--------1000 is saved with 13 steps. V_table and Action table\n","./results_DQN/01-08_(6, 6)_GAM_0.9_NE_1000_TU_100_END_0.15_1000000_BS_2048/result_01-08_06:33:01 is done\n","Complete\n"]}]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"8yIU7zzPtXVv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir=./"],"metadata":{"id":"2rw4HMF163YB"},"execution_count":null,"outputs":[]}]}